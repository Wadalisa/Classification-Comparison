{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GawbJBgFnutw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "TYP_ZIP_FILE = 'typ.zip'\n",
    "EXO_ZIP_FILE = 'exo.zip'\n",
    "LABELS_FILE = 'labels.csv'\n",
    "TEST_FILE = 'test.csv'\n",
    "OUTPUT_FILE = 'cleaned_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit CPU threads for PyTorch\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "# Also tell PyTorch explicitly\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T2dqEfwVpDJ9"
   },
   "outputs": [],
   "source": [
    "def get_image_file_data(zip_files):\n",
    "\n",
    "    image_data = []\n",
    "\n",
    "    coord_pattern = re.compile(r'^([\\d\\.-]+)\\s([\\d\\.-]+)_\\[.*\\.png$')\n",
    "\n",
    "    print(f\"Scanning zip files: {zip_files}\")\n",
    "    for zip_name in zip_files:\n",
    "        if not os.path.exists(zip_name):\n",
    "            print(f\"Warning: Zip file not found: {zip_name}\")\n",
    "            continue\n",
    "\n",
    "        with zipfile.ZipFile(zip_name, 'r') as zf:\n",
    "            for filepath in zf.namelist():\n",
    "\n",
    "                if filepath.endswith('/') or '.ipynb_checkpoints' in filepath or '__MACOSX' in filepath:\n",
    "                    continue\n",
    "\n",
    "                filename = os.path.basename(filepath)\n",
    "\n",
    "                match = coord_pattern.match(filename)\n",
    "\n",
    "                if match:\n",
    "                    ra_str, dec_str = match.groups()\n",
    "                    try:\n",
    "                        ra = float(ra_str)\n",
    "                        dec = float(dec_str)\n",
    "                        image_data.append((filepath, ra, dec))\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not parse coordinates from: {filepath}\")\n",
    "\n",
    "    if not image_data:\n",
    "        print(\"Error: No image files with valid coordinates found. Stopping.\")\n",
    "        return None\n",
    "\n",
    "    image_df = pd.DataFrame(image_data, columns=['image_filepath', 'img_ra', 'img_dec'])\n",
    "    print(f\"Found {len(image_df)} images in zip files.\")\n",
    "    return image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "E1-bdPuhpRRg",
    "outputId": "d26002cf-ef91-436e-cf08-da0d11c08b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning zip files: ['typ.zip', 'exo.zip']\n",
      "Found 2107 images in zip files.\n",
      "Built KDTree from image file coordinates.\n",
      "--- Head of Mapped Labels (with match distance) ---\n",
      "     label_ra  label_dec             2    3    4  \\\n",
      "0   10.328221 -20.476357         FR II  NaN  NaN   \n",
      "1   92.109802 -49.431413       typical  NaN  NaN   \n",
      "2   88.916825 -59.431868  Point Source  NaN  NaN   \n",
      "3    5.457981 -25.589637         FR II  NaN  NaN   \n",
      "4  119.417608 -53.396711         FR II  NaN  NaN   \n",
      "\n",
      "                                      image_filepath  match_distance  \n",
      "0  typ_PNG/10.328 -20.476_[0.01424399 0.01424399]...        0.000420  \n",
      "1  typ_PNG/92.110 -49.431_[0.04121882 0.04121882]...        0.000458  \n",
      "2  typ_PNG/88.917 -59.432_[0.009 0.009] deg_(J060...        0.000219  \n",
      "3  typ_PNG/5.458 -25.590_[0.009 0.009] deg_(Abell...        0.000363  \n",
      "4  typ_PNG/119.418 -53.397_[0.009 0.009] deg_(J07...        0.000487  \n",
      "Average match distance: 0.0153\n",
      "\n",
      "Found 9 unique classes.\n",
      "\n",
      "--- Final Cleaned Data Head ---\n",
      "                                      image_filepath    label_ra  label_dec  \\\n",
      "0  exo_PNG/0.523 -24.568_[0.04242756 0.04242756] ...    0.522615 -24.568379   \n",
      "1  exo_PNG/1.028 -25.064_[0.07124072 0.07124072] ...    1.027953 -25.064459   \n",
      "2  exo_PNG/1.076 -24.429_[0.03492131 0.03492131] ...    1.076097 -24.429189   \n",
      "3  exo_PNG/1.448 -24.492_[0.04284089 0.04284089] ...    1.447591 -24.491664   \n",
      "4  exo_PNG/108.614 -60.373_[0.11265306 0.11265306...  108.613797 -60.373366   \n",
      "\n",
      "   match_distance  Bent  Exotic  FR I  FR II  Point Source  S/Z shaped  \\\n",
      "0        0.000541     0       1     0      0             0           0   \n",
      "1        0.000462     0       1     0      0             0           0   \n",
      "2        0.000212     0       1     0      0             0           0   \n",
      "3        0.000530     0       1     0      0             0           0   \n",
      "4        0.000419     1       1     0      0             0           0   \n",
      "\n",
      "   Should be discarded  X-Shaped  typical  \n",
      "0                    0         0        0  \n",
      "1                    0         0        0  \n",
      "2                    0         0        0  \n",
      "3                    0         0        0  \n",
      "4                    0         0        0  \n",
      "\n",
      "Successfully cleaned and saved data to 'cleaned_labels.csv'.\n",
      "Final shape: (2178, 13)\n",
      "Classes found: ['Bent', 'Exotic', 'FR I', 'FR II', 'Point Source', 'S/Z shaped', 'Should be discarded', 'X-Shaped', 'typical']\n"
     ]
    }
   ],
   "source": [
    "def run_cleaning_pipeline():\n",
    "    image_df = get_image_file_data([TYP_ZIP_FILE, EXO_ZIP_FILE])\n",
    "    if image_df is None:\n",
    "        return\n",
    "\n",
    "\n",
    "    coords_from_images = image_df[['img_ra', 'img_dec']].values\n",
    "    kdtree = KDTree(coords_from_images)\n",
    "    print(\"Built KDTree from image file coordinates.\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(LABELS_FILE):\n",
    "        print(f\"Error: Labels file not found: {LABELS_FILE}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    labels_df = pd.read_csv(LABELS_FILE, header=None)\n",
    "\n",
    "\n",
    "    labels_df = labels_df.rename(columns={0: 'label_ra', 1: 'label_dec'})\n",
    "\n",
    "\n",
    "    label_coords = labels_df[['label_ra', 'label_dec']].values\n",
    "\n",
    "\n",
    "    distances, indices = kdtree.query(label_coords)\n",
    "\n",
    "\n",
    "    labels_df['image_filepath'] = image_df.iloc[indices]['image_filepath'].values\n",
    "    labels_df['match_distance'] = distances\n",
    "\n",
    "    print(\"--- Head of Mapped Labels (with match distance) ---\")\n",
    "    print(labels_df.head())\n",
    "    print(f\"Average match distance: {labels_df['match_distance'].mean():.4f}\")\n",
    "\n",
    "\n",
    "    id_vars = ['image_filepath', 'label_ra', 'label_dec', 'match_distance']\n",
    "    label_cols = [col for col in labels_df.columns if col not in id_vars]\n",
    "\n",
    "\n",
    "    long_df = pd.melt(\n",
    "        labels_df,\n",
    "        id_vars=id_vars,\n",
    "        value_vars=label_cols,\n",
    "        value_name='label'\n",
    "    )\n",
    "\n",
    "\n",
    "    long_df = long_df.dropna(subset=['label'])\n",
    "\n",
    "    long_df = long_df.drop(columns=['variable'])\n",
    "\n",
    "    print(f\"\\nFound {long_df['label'].nunique()} unique classes.\")\n",
    "\n",
    "\n",
    "    encoded_df = pd.get_dummies(long_df, columns=['label'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "    final_df = encoded_df.groupby(\n",
    "        ['image_filepath', 'label_ra', 'label_dec', 'match_distance']\n",
    "    ).sum().reset_index()\n",
    "\n",
    "\n",
    "    label_cols = [col for col in final_df.columns if col not in id_vars]\n",
    "    final_df[label_cols] = final_df[label_cols].astype(int)\n",
    "\n",
    "\n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"\\n--- Final Cleaned Data Head ---\")\n",
    "    print(final_df.head())\n",
    "    print(f\"\\nSuccessfully cleaned and saved data to '{OUTPUT_FILE}'.\")\n",
    "    print(f\"Final shape: {final_df.shape}\")\n",
    "    print(f\"Classes found: {label_cols}\")\n",
    "\n",
    "run_cleaning_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ROIUjDHZp2Y-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Test File Pipeline ---\n",
      "Scanning zip files: ['typ.zip', 'exo.zip']\n",
      "Found 2107 images in zip files.\n",
      "Built KDTree from image file coordinates.\n",
      "--- Head of Mapped Test Images ---\n",
      "      test_ra   test_dec                                     image_filepath  \\\n",
      "0  201.743657 -31.321637  typ_PNG/201.744 -31.322_[0.03007117 0.03007117...   \n",
      "1  234.261286 -46.590846  typ_PNG/234.261 -46.591_[0.01291882 0.01291882...   \n",
      "2   66.793081 -62.375058  typ_PNG/66.793 -62.375_[0.009 0.009] deg_(J043...   \n",
      "3  108.760518 -59.958776  typ_PNG/108.761 -59.959_[0.009 0.009] deg_(J07...   \n",
      "4  202.148240 -31.432391  typ_PNG/202.148 -31.432_[0.01254835 0.01254835...   \n",
      "\n",
      "   match_distance  \n",
      "0        0.000499  \n",
      "1        0.000325  \n",
      "2        0.000100  \n",
      "3        0.000532  \n",
      "4        0.000459  \n",
      "Average match distance: 0.0154\n",
      "\n",
      "Successfully mapped test coordinates and saved to 'test_images_to_predict.csv'.\n",
      "Final shape: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "TYP_ZIP_FILE = 'typ.zip'\n",
    "EXO_ZIP_FILE = 'exo.zip'\n",
    "TEST_FILE = 'test.csv'\n",
    "OUTPUT_FILE = 'test_images_to_predict.csv'\n",
    "\n",
    "def get_image_file_data(zip_files):\n",
    "\n",
    "    image_data = []\n",
    "\n",
    "    coord_pattern = re.compile(r'^([\\d\\.-]+)\\s([\\d\\.-]+)_\\[.*\\.png$')\n",
    "\n",
    "    print(f\"Scanning zip files: {zip_files}\")\n",
    "    for zip_name in zip_files:\n",
    "        if not os.path.exists(zip_name):\n",
    "            print(f\"Warning: Zip file not found: {zip_name}\")\n",
    "            continue\n",
    "\n",
    "        with zipfile.ZipFile(zip_name, 'r') as zf:\n",
    "            for filepath in zf.namelist():\n",
    "\n",
    "                if filepath.endswith('/') or '.ipynb_checkpoints' in filepath or '__MACOSX' in filepath:\n",
    "                    continue\n",
    "\n",
    "                filename = os.path.basename(filepath)\n",
    "\n",
    "                match = coord_pattern.match(filename)\n",
    "\n",
    "                if match:\n",
    "\n",
    "                    ra_str, dec_str = match.groups()\n",
    "                    try:\n",
    "                        ra = float(ra_str)\n",
    "                        dec = float(dec_str)\n",
    "\n",
    "                        image_data.append((filepath, ra, dec))\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not parse coordinates from: {filepath}\")\n",
    "\n",
    "    if not image_data:\n",
    "        print(\"Error: No image files with valid coordinates found. Stopping.\")\n",
    "        return None\n",
    "\n",
    "    image_df = pd.DataFrame(image_data, columns=['image_filepath', 'img_ra', 'img_dec'])\n",
    "    print(f\"Found {len(image_df)} images in zip files.\")\n",
    "    return image_df\n",
    "\n",
    "def run_test_pipeline():\n",
    "    \"\"\"\n",
    "    Executes the test file processing pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Test File Pipeline ---\")\n",
    "\n",
    "\n",
    "    image_df = get_image_file_data([TYP_ZIP_FILE, EXO_ZIP_FILE])\n",
    "    if image_df is None:\n",
    "        return\n",
    "\n",
    "    coords_from_images = image_df[['img_ra', 'img_dec']].values\n",
    "    kdtree = KDTree(coords_from_images)\n",
    "    print(\"Built KDTree from image file coordinates.\")\n",
    "\n",
    "    if not os.path.exists(TEST_FILE):\n",
    "        print(f\"Error: Test file not found: {TEST_FILE}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    test_df = pd.read_csv(TEST_FILE, header=None)\n",
    "\n",
    "\n",
    "    test_df = test_df.rename(columns={0: 'test_ra', 1: 'test_dec'})\n",
    "\n",
    "    test_coords = test_df[['test_ra', 'test_dec']].values\n",
    "\n",
    "\n",
    "    distances, indices = kdtree.query(test_coords)\n",
    "\n",
    "\n",
    "    test_df['image_filepath'] = image_df.iloc[indices]['image_filepath'].values\n",
    "    test_df['match_distance'] = distances\n",
    "\n",
    "    print(\"--- Head of Mapped Test Images ---\")\n",
    "    print(test_df.head())\n",
    "    print(f\"Average match distance: {test_df['match_distance'].mean():.4f}\")\n",
    "\n",
    "\n",
    "    test_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully mapped test coordinates and saved to '{OUTPUT_FILE}'.\")\n",
    "    print(f\"Final shape: {test_df.shape}\")\n",
    "\n",
    "run_test_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DoV-hnYFqeB1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wadalisam\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 9 classes: ['Bent', 'Exotic', 'FR I', 'FR II', 'Point Source', 'S/Z shaped', 'Should be discarded', 'X-Shaped', 'typical']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "LABELS_FILE = 'cleaned_labels.csv'\n",
    "IMAGE_ROOT_DIR = 'images/'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "temp_df = pd.read_csv(LABELS_FILE)\n",
    "id_cols = ['image_filepath', 'label_ra', 'label_dec', 'match_distance']\n",
    "CLASS_NAMES = [col for col in temp_df.columns if col not in id_cols]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"Found {NUM_CLASSES} classes: {CLASS_NAMES}\")\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GDOueHjVqtA0"
   },
   "outputs": [],
   "source": [
    "class RadioSourceDataset(Dataset):\n",
    "    def __init__(self, df, image_root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.transform = transform\n",
    "        self.labels = df[CLASS_NAMES].values\n",
    "        self.filepaths = df['image_filepath'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.filepaths[idx]\n",
    "\n",
    "\n",
    "        full_path = os.path.join(self.image_root_dir, img_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            img = Image.open(full_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image {full_path}: {e}\")\n",
    "\n",
    "            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color='black')\n",
    "\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sO-0dOb2qxB2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1742\n",
      "Validation samples: 436\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "df = pd.read_csv(LABELS_FILE)\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eY1HObtgq3Ay"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated *DAMPENED* positive class weights:\n",
      "- Bent: 1.81\n",
      "- Exotic: 5.94\n",
      "- FR I: 2.02\n",
      "- FR II: 1.46\n",
      "- Point Source: 1.97\n",
      "- S/Z shaped: 9.79\n",
      "- Should be discarded: 3.18\n",
      "- X-Shaped: 18.64\n",
      "- typical: 9.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_counts = train_df[CLASS_NAMES].sum()\n",
    "neg_counts = len(train_df) - pos_counts\n",
    "\n",
    "pos_weight = (neg_counts / (pos_counts + 1e-6)) ** 0.5\n",
    "\n",
    "pos_weight_tensor = torch.tensor(pos_weight.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "print(\"Calculated *DAMPENED* positive class weights:\")\n",
    "for name, weight in zip(CLASS_NAMES, pos_weight_tensor):\n",
    "    print(f\"- {name}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KKbHK6GLq7xe"
   },
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    num_ftrs = model.classifier.in_features\n",
    "\n",
    "\n",
    "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "f1_metric = torchmetrics.F1Score(task=\"multilabel\", num_labels=NUM_CLASSES, average='macro').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fERgnNWrDXl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training (on unzipped files) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchmetrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(LABELS_FILE)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = RadioSourceDataset(train_df, IMAGE_ROOT_DIR, transform=train_transform)\n",
    "val_dataset = RadioSourceDataset(val_df, IMAGE_ROOT_DIR, transform=val_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"\\n--- Starting Training (on unzipped files) ---\")\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "model_save_path = \"best_model_v3.pth\"\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model(NUM_CLASSES).to(DEVICE)\n",
    "pos_weight_tensor = torch.tensor(pos_weight.values, dtype=torch.float32).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "f1_metric = torchmetrics.F1Score(task=\"multilabel\", num_labels=NUM_CLASSES, average='macro').to(DEVICE)\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            f1_metric.update(preds, labels.int())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_f1 = f1_metric.compute()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append(val_f1.cpu().item())\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"*** New best model saved to {model_save_path} (F1: {best_val_f1:.4f}) ***\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best validation Macro F1 score: {best_val_f1:.4f}\")\n",
    "print(f\"Best model weights saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Plotting Training Performance ---\")\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_loss_history, 'b-', label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss_history, 'r-', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, val_f1_history, 'g-', label='Validation Macro F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Validation Macro F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Generating Confusion Matrix for Best Model ({model_save_path})\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "model = create_model(NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "all_preds_binary = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in tqdm(val_loader, desc=\"Running final validation for CM\"):\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        logits = model(images)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds_binary = (probs > 0.5).cpu().numpy().astype(int)\n",
    "\n",
    "        all_preds_binary.extend(preds_binary)\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "all_preds_binary = np.array(all_preds_binary)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "ml_cm = multilabel_confusion_matrix(all_labels, all_preds_binary)\n",
    "\n",
    "\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = (num_classes + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (matrix, name) in enumerate(zip(ml_cm, CLASS_NAMES)):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Neg', 'Pred Pos'],\n",
    "                yticklabels=['True Neg', 'True Pos'],\n",
    "                ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix for: {name}')\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eu4qLHN4BJTR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- NEW IMPORTS ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# --------------------\n",
    "\n",
    "UNL_DIR = 'images_unl/'\n",
    "UNL_ZIP = 'unl.zip'\n",
    "if not os.path.exists(UNL_DIR):\n",
    "    print(f\"Creating directory {UNL_DIR}\")\n",
    "    os.makedirs(UNL_DIR, exist_ok=True)\n",
    "    print(f\"Unzipping {UNL_ZIP}...\")\n",
    "    !unzip -q {UNL_ZIP} -d {UNL_DIR}\n",
    "    print(\"Unzipping complete.\")\n",
    "else:\n",
    "    print(f\"Directory {UNL_DIR} already exists. Skipping unzip.\")\n",
    "\n",
    "\n",
    "def get_all_unl_images(unl_dir):\n",
    "    \"\"\"\n",
    "    Robustly walks all subdirectories to find image files\n",
    "    and parse their coordinates.\n",
    "    \"\"\"\n",
    "    image_data = []\n",
    "    coord_pattern = re.compile(r'^([\\d\\.-]+)\\s([\\d\\.-]+)_\\[.*\\.png$')\n",
    "\n",
    "    print(f\"Scanning all subfolders in {unl_dir}...\")\n",
    "    for root, _, files in os.walk(unl_dir):\n",
    "        for filename in files:\n",
    "            if '.ipynb_checkpoints' in root or '__MACOSX' in root or not filename.endswith('.png'):\n",
    "                continue\n",
    "            match = coord_pattern.match(filename)\n",
    "            if match:\n",
    "                ra_str, dec_str = match.groups()\n",
    "                try:\n",
    "                    ra = float(ra_str)\n",
    "                    dec = float(dec_str)\n",
    "                    full_path = os.path.join(root, filename)\n",
    "                    rel_path = os.path.relpath(full_path, unl_dir)\n",
    "                    image_data.append((rel_path, ra, dec))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not parse coordinates from: {filename}\")\n",
    "\n",
    "    if not image_data:\n",
    "        print(\"Error: No image files with valid coordinates found. Stopping.\")\n",
    "        return None\n",
    "\n",
    "    image_df = pd.DataFrame(image_data, columns=['image_filepath', 'img_ra', 'img_dec'])\n",
    "    print(f\"Found {len(image_df)} images. (Expected: 13821)\")\n",
    "    return image_df\n",
    "\n",
    "\n",
    "MODEL_FILE = 'best_model_v3.pth'\n",
    "UNL_MAP_FILE = 'unl_images_to_predict_FULL.csv'\n",
    "SUBMISSION_FILE = 'generated_labels_FULL.csv'\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "temp_df = pd.read_csv('cleaned_labels.csv')\n",
    "id_cols = ['image_filepath', 'label_ra', 'label_dec', 'match_distance']\n",
    "CLASS_NAMES = [col for col in temp_df.columns if col not in id_cols]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "del temp_df\n",
    "print(f\"Loaded {NUM_CLASSES} classes: {CLASS_NAMES}\")\n",
    "\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=False)\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "class UnlRadioSourceDataset(Dataset):\n",
    "    def __init__(self, df, image_root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.transform = transform\n",
    "        self.filepaths = df['image_filepath'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filepaths[idx]\n",
    "        full_path = os.path.join(self.image_root_dir, img_path)\n",
    "        try:\n",
    "            img = Image.open(full_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color='black')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "print(f\"--- Generating {SUBMISSION_FILE} ---\")\n",
    "\n",
    "unl_map_df = get_all_unl_images(UNL_DIR)\n",
    "if unl_map_df is not None:\n",
    "    unl_map_df.to_csv(UNL_MAP_FILE, index=False)\n",
    "\n",
    "    print(f\"Loading model from {MODEL_FILE}...\")\n",
    "    model = create_model(NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(MODEL_FILE, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    unl_dataset = UnlRadioSourceDataset(unl_map_df, UNL_DIR, transform=val_transform)\n",
    "    unl_loader = DataLoader(unl_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    all_preds_binary = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(unl_loader, desc=\"Generating Predictions\"):\n",
    "            images = images.to(DEVICE)\n",
    "            logits = model(images)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            # --- We'll use a 0.5 threshold for this. ---\n",
    "            # You can change this based on your threshold analysis!\n",
    "            preds = (probs > 0.5).cpu().numpy().astype(int)\n",
    "            all_preds_binary.extend(preds)\n",
    "\n",
    "    print(\"Formatting submission file...\")\n",
    "    preds_array = np.array(all_preds_binary)\n",
    "    coordinates = unl_map_df[['img_ra', 'img_dec']].values\n",
    "    submission_data = []\n",
    "\n",
    "    for (ra, dec), binary_preds in zip(coordinates, preds_array):\n",
    "        predicted_labels = [CLASS_NAMES[i] for i, val in enumerate(binary_preds) if val == 1]\n",
    "        row = [ra, dec] + predicted_labels\n",
    "        submission_data.append(row)\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(SUBMISSION_FILE, header=False, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved predictions to '{SUBMISSION_FILE}'\")\n",
    "    print(f\"Total labels generated: {len(submission_df)}\")\n",
    "    print(\"--- Head of submission file ---\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "\n",
    "    # ==================================================================\n",
    "    # --- NEW: VISUALIZATION SECTION ---\n",
    "    # ==================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Generating Visualizations for Unlabelled Set\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- Visualization 1: Bar Chart of Predicted Labels ---\n",
    "    print(\"Plotting predicted label distribution...\")\n",
    "\n",
    "    # Melt the submission dataframe to count labels\n",
    "    # Use 0 and 1 as column names since there's no header\n",
    "    id_vars = [0, 1]\n",
    "    label_cols = [col for col in submission_df.columns if col not in id_vars]\n",
    "\n",
    "    if not label_cols:\n",
    "        print(\"Warning: Model did not predict any labels.\")\n",
    "    else:\n",
    "        melted_df = submission_df.melt(\n",
    "            id_vars=id_vars,\n",
    "            value_vars=label_cols,\n",
    "            value_name='label'\n",
    "        ).dropna(subset=['label'])\n",
    "\n",
    "        label_counts = melted_df['label'].value_counts()\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.barplot(x=label_counts.index, y=label_counts.values, order=label_counts.index)\n",
    "        plt.title('Predicted Label Distribution (for unl.zip)', fontsize=16)\n",
    "        plt.ylabel('Total Count', fontsize=12)\n",
    "        plt.xlabel('Class', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # --- Visualization 2: Sample Image Predictions ---\n",
    "    print(\"\\nPlotting sample predictions...\")\n",
    "\n",
    "    def format_labels(binary_vec, names):\n",
    "        labels = [names[i] for i, val in enumerate(binary_vec) if val == 1]\n",
    "        if not labels:\n",
    "            return \"None\"\n",
    "        return \", \".join(labels)\n",
    "\n",
    "    # Get 9 random samples from the unlabelled set\n",
    "    sample_indices = np.random.choice(len(unl_map_df), 9, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 17))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Get image path\n",
    "        filepath = unl_map_df.iloc[idx]['image_filepath']\n",
    "        full_path = os.path.join(UNL_DIR, filepath)\n",
    "\n",
    "        # Get predicted labels\n",
    "        binary_preds = preds_array[idx] # From our prediction loop\n",
    "        pred_labels_str = format_labels(binary_preds, CLASS_NAMES)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(full_path).convert('RGB')\n",
    "        except Exception:\n",
    "            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color='black') # Fallback\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f\"Predicted: {pred_labels_str}\", color='blue', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Could not find any unlabelled images. Pipeline stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le09fGcwBZw1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "MANUAL_LABELS_FILE = 'cleaned_labels.csv'\n",
    "PSEUDO_LABELS_FILE = 'generated_labels_FULL.csv'\n",
    "UNL_MAP_FILE = 'unl_images_to_predict_FULL.csv'\n",
    "\n",
    "\n",
    "IMAGE_ROOT_DIR_LBL = '/content/images/'\n",
    "IMAGE_ROOT_DIR_UNL = 'images_unl/'\n",
    "\n",
    "\n",
    "MODEL_TO_FINETUNE = 'best_model_v3.pth'\n",
    "FINAL_MODEL_SAVE_PATH = 'best_model_v4_pseudo.pth'\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "temp_df = pd.read_csv(MANUAL_LABELS_FILE)\n",
    "id_cols = ['image_filepath', 'label_ra', 'label_dec', 'match_distance']\n",
    "CLASS_NAMES = [col for col in temp_df.columns if col not in id_cols]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "del temp_df\n",
    "print(f\"Loaded {NUM_CLASSES} classes: {CLASS_NAMES}\")\n",
    "\n",
    "\n",
    "print(\"Loading and processing pseudo-labels...\")\n",
    "unl_map_df = pd.read_csv(UNL_MAP_FILE)\n",
    "\n",
    "\n",
    "pseudo_df = pd.read_csv(PSEUDO_LABELS_FILE, header=None)\n",
    "pseudo_df = pseudo_df.rename(columns={0: 'img_ra', 1: 'img_dec'})\n",
    "\n",
    "\n",
    "id_vars = ['img_ra', 'img_dec']\n",
    "label_cols = [col for col in pseudo_df.columns if col not in id_vars]\n",
    "\n",
    "if not label_cols:\n",
    "    print(\"Warning: Your pseudo-label file appears to have no labels.\")\n",
    "    pseudo_hot_df = pd.DataFrame(columns=['img_ra', 'img_dec'] + CLASS_NAMES)\n",
    "else:\n",
    "    long_df = pd.melt(pseudo_df, id_vars=id_vars, value_vars=label_cols, value_name='label').dropna(subset=['label'])\n",
    "\n",
    "    encoded_df = pd.get_dummies(long_df, columns=['label'], prefix='', prefix_sep='')\n",
    "\n",
    "    pseudo_hot_df = encoded_df.groupby(['img_ra', 'img_dec']).sum().reset_index()\n",
    "\n",
    "\n",
    "pseudo_hot_df['img_ra_round'] = pseudo_hot_df['img_ra'].round(5)\n",
    "unl_map_df['img_ra_round'] = unl_map_df['img_ra'].round(5)\n",
    "pseudo_hot_df['img_dec_round'] = pseudo_hot_df['img_dec'].round(5)\n",
    "unl_map_df['img_dec_round'] = unl_map_df['img_dec'].round(5)\n",
    "\n",
    "final_pseudo_df = pd.merge(\n",
    "    unl_map_df,\n",
    "    pseudo_hot_df,\n",
    "    on=['img_ra_round', 'img_dec_round'],\n",
    "    how='left',\n",
    "    suffixes=('_map', '_pseudo')\n",
    ")\n",
    "\n",
    "\n",
    "for col in CLASS_NAMES:\n",
    "    if col not in final_pseudo_df.columns:\n",
    "        final_pseudo_df[col] = 0\n",
    "\n",
    "final_pseudo_df[CLASS_NAMES] = final_pseudo_df[CLASS_NAMES].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "final_pseudo_df = final_pseudo_df[['image_filepath'] + CLASS_NAMES]\n",
    "print(f\"Loaded {len(final_pseudo_df)} pseudo-labeled samples.\")\n",
    "\n",
    "\n",
    "class RadioSourceDataset(Dataset):\n",
    "    def __init__(self, df, image_root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.transform = transform\n",
    "        self.labels = df[CLASS_NAMES].values\n",
    "        self.filepaths = df['image_filepath'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filepaths[idx]\n",
    "        full_path = os.path.join(self.image_root_dir, img_path)\n",
    "        try:\n",
    "            img = Image.open(full_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color='black')\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        else:\n",
    "            img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "df_labeled = pd.read_csv(MANUAL_LABELS_FILE)\n",
    "train_df, val_df = train_test_split(df_labeled, test_size=0.2, random_state=42)\n",
    "original_train_dataset = RadioSourceDataset(train_df, IMAGE_ROOT_DIR_LBL, transform=train_transform)\n",
    "val_dataset = RadioSourceDataset(val_df, IMAGE_ROOT_DIR_LBL, transform=val_transform)\n",
    "\n",
    "pseudo_dataset = RadioSourceDataset(final_pseudo_df, IMAGE_ROOT_DIR_UNL, transform=train_transform)\n",
    "\n",
    "\n",
    "combined_train_dataset = ConcatDataset([original_train_dataset, pseudo_dataset])\n",
    "\n",
    "print(f\"Original training samples: {len(original_train_dataset)}\")\n",
    "print(f\"Pseudo-labeled samples: {len(pseudo_dataset)}\")\n",
    "print(f\"Total combined training samples: {len(combined_train_dataset)}\")\n",
    "print(f\"Validation samples (unchanged): {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=False)\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_TO_FINETUNE, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "pos_counts = train_df[CLASS_NAMES].sum()\n",
    "neg_counts = len(train_df)\n",
    "pos_weight = (neg_counts / (pos_counts + 1e-6)) ** 0.5\n",
    "pos_weight_tensor = torch.tensor(pos_weight.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "f1_metric = torchmetrics.F1Score(task=\"multilabel\", num_labels=NUM_CLASSES, average='macro').to(DEVICE)\n",
    "\n",
    "print(\"\\n--- Starting Pseudo-Label Fine-Tuning ---\")\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    f1_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            f1_metric.update(preds, labels.int())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_f1 = f1_metric.compute()\n",
    "\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append(val_f1.cpu().item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f} - \"\n",
    "          f\"Val Loss: {val_loss:.4f} - \"\n",
    "          f\"Val Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), FINAL_MODEL_SAVE_PATH)\n",
    "        print(f\"*** New best model saved to {FINAL_MODEL_SAVE_PATH} (F1: {best_val_f1:.4f}) ***\")\n",
    "\n",
    "print(\"\\n--- Pseudo-Label Training Complete ---\")\n",
    "print(f\"Best validation Macro F1 score: {best_val_f1:.4f}\")\n",
    "print(f\"Final model weights saved to {FINAL_MODEL_SAVE_PATH}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Plotting Final Training Performance ---\")\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(epochs_range, train_loss_history, 'b-', label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss_history, 'r-', label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Final Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, val_f1_history, 'g-', label='Validation Macro F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Final Validation Macro F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
